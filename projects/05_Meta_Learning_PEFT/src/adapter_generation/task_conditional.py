"""Task-conditional adapter generation using HyperNetworks.

This module implements HyperNetworks that generate adapter parameters
conditioned on task embeddings, enabling task-specific adapter customization.

Reference:
    Ha et al. (2016) "HyperNetworks"
"""

from __future__ import annotations

from typing import Any

import mlx.core as mx
import mlx.nn as nn

from task_embedding.learned_embeddings import TaskEmbeddingNetwork
from utils.logging import get_logger

logger = get_logger(__name__)


class LoRAHyperNetwork(nn.Module):
    """HyperNetwork that generates LoRA adapter parameters from task embeddings.

    Given a task embedding, this network outputs the low-rank matrices
    (A and B) for a LoRA adapter.

    Attributes:
        task_embedding_dim: Dimension of task embeddings
        in_features: Input dimension for generated LoRA
        out_features: Output dimension for generated LoRA
        rank: Rank of generated LoRA
    """

    def __init__(
        self,
        task_embedding_dim: int = 64,
        in_features: int = 128,
        out_features: int = 128,
        rank: int = 8,
        hidden_dim: int = 256,
    ):
        """Initialize LoRA HyperNetwork.

        Args:
            task_embedding_dim: Task embedding dimension
            in_features: LoRA input dimension
            out_features: LoRA output dimension
            rank: LoRA rank
            hidden_dim: Hidden layer dimension for HyperNetwork
        """
        super().__init__()
        self.task_embedding_dim = task_embedding_dim
        self.in_features = in_features
        self.out_features = out_features
        self.rank = rank

        # Network to generate LoRA_A (rank x in_features)
        a_params = rank * in_features
        self.generate_A = nn.Sequential(
            nn.Linear(task_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, a_params),
        )

        # Network to generate LoRA_B (out_features x rank)
        b_params = out_features * rank
        self.generate_B = nn.Sequential(
            nn.Linear(task_embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, b_params),
        )

    def __call__(self, task_embedding: mx.array) -> tuple[mx.array, mx.array]:
        """Generate LoRA parameters from task embedding.

        Args:
            task_embedding: Task embedding of shape (task_embedding_dim,)
                           or (batch_size, task_embedding_dim)

        Returns:
            Tuple of (lora_A, lora_B):
                - lora_A: (rank, in_features)
                - lora_B: (out_features, rank)
        """
        # Generate A
        a_flat = self.generate_A(task_embedding)
        lora_A = mx.reshape(a_flat, (self.rank, self.in_features))

        # Generate B
        b_flat = self.generate_B(task_embedding)
        lora_B = mx.reshape(b_flat, (self.out_features, self.rank))

        return lora_A, lora_B


class TaskConditionalLoRALayer(nn.Module):
    """LoRA layer with task-conditional parameters.

    The LoRA parameters (A and B) are generated by a HyperNetwork
    based on the task embedding.
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        rank: int = 8,
        alpha: float = 16.0,
        task_embedding_dim: int = 64,
    ):
        """Initialize task-conditional LoRA layer.

        Args:
            in_features: Input dimension
            out_features: Output dimension
            rank: LoRA rank
            alpha: Scaling factor
            task_embedding_dim: Task embedding dimension
        """
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.rank = rank
        self.scaling = alpha / rank

        # Frozen base weight
        self.weight = mx.random.normal((out_features, in_features)) * 0.01

        # HyperNetwork to generate LoRA parameters
        self.hyper_network = LoRAHyperNetwork(
            task_embedding_dim=task_embedding_dim,
            in_features=in_features,
            out_features=out_features,
            rank=rank,
        )

        # Cache for current task's LoRA parameters
        self._cached_lora_A = None
        self._cached_lora_B = None

    def set_task_embedding(self, task_embedding: mx.array) -> None:
        """Set task embedding and generate corresponding LoRA parameters.

        Args:
            task_embedding: Task embedding vector
        """
        # Generate task-specific LoRA parameters
        self._cached_lora_A, self._cached_lora_B = self.hyper_network(
            task_embedding
        )

    def __call__(self, x: mx.array) -> mx.array:
        """Forward pass with task-conditional LoRA.

        Args:
            x: Input tensor (..., in_features)

        Returns:
            Output tensor (..., out_features)
        """
        # Base forward pass
        base_output = x @ self.weight.T

        # LoRA forward with task-conditional parameters
        if self._cached_lora_A is not None and self._cached_lora_B is not None:
            lora_output = (x @ self._cached_lora_A.T) @ self._cached_lora_B.T
            lora_output = lora_output * self.scaling
            return base_output + lora_output
        else:
            # No task embedding set, use base only
            return base_output


class TaskConditionalAdapterModel(nn.Module):
    """Model with task-conditional adapters.

    Uses HyperNetworks to generate task-specific adapter parameters
    based on task embeddings.
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        output_dim: int,
        lora_rank: int = 8,
        lora_alpha: float = 16.0,
        task_embedding_dim: int = 64,
    ):
        """Initialize task-conditional adapter model.

        Args:
            input_dim: Input dimension
            hidden_dim: Hidden layer dimension
            output_dim: Output dimension
            lora_rank: LoRA rank
            lora_alpha: Scaling factor
            task_embedding_dim: Task embedding dimension
        """
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim

        # Task-conditional LoRA layers
        self.layer1 = TaskConditionalLoRALayer(
            input_dim,
            hidden_dim,
            rank=lora_rank,
            alpha=lora_alpha,
            task_embedding_dim=task_embedding_dim,
        )
        self.layer2 = TaskConditionalLoRALayer(
            hidden_dim,
            output_dim,
            rank=lora_rank,
            alpha=lora_alpha,
            task_embedding_dim=task_embedding_dim,
        )

        # Task embedding network (shared)
        self.task_embedder = TaskEmbeddingNetwork(
            input_dim=128,  # Dataset features
            hidden_dim=256,
            embedding_dim=task_embedding_dim,
        )

    def set_task(
        self,
        task_features: mx.array | None = None,
        task_embedding: mx.array | None = None,
    ) -> None:
        """Set current task by computing or providing task embedding.

        Args:
            task_features: Task features to compute embedding (alternative to task_embedding)
            task_embedding: Pre-computed task embedding (alternative to task_features)
        """
        if task_embedding is not None:
            embedding = task_embedding
        elif task_features is not None:
            embedding = self.task_embedder(task_features)
        else:
            raise ValueError(
                "Must provide either task_features or task_embedding"
            )

        # Set task embedding for all layers
        self.layer1.set_task_embedding(embedding)
        self.layer2.set_task_embedding(embedding)

    def __call__(self, x: mx.array) -> mx.array:
        """Forward pass.

        Args:
            x: Input tensor (batch_size, input_dim)

        Returns:
            Logits (batch_size, output_dim)
        """
        x = self.layer1(x)
        x = nn.relu(x)
        x = self.layer2(x)
        return x


class AdapterHyperparameterOptimizer:
    """Optimize adapter hyperparameters using Bayesian optimization.

    Finds optimal rank, alpha, and other hyperparameters for PEFT
    methods based on task characteristics.
    """

    def __init__(
        self,
        param_ranges: dict[str, tuple[float, float]] | None = None,
    ):
        """Initialize hyperparameter optimizer.

        Args:
            param_ranges: Dictionary of parameter ranges
                         e.g., {"rank": (4, 32), "alpha": (8.0, 32.0)}
        """
        self.param_ranges = param_ranges or {
            "rank": (4, 32),
            "alpha": (8.0, 32.0),
            "dropout": (0.0, 0.3),
        }

        # History of trials
        self.trial_history: list[dict[str, Any]] = []

    def suggest_hyperparameters(
        self, task_embedding: mx.array | None = None
    ) -> dict[str, Any]:
        """Suggest hyperparameters for current task.

        Args:
            task_embedding: Optional task embedding for task-conditional suggestions

        Returns:
            Dictionary of suggested hyperparameters
        """
        # Simple heuristic-based suggestion (could be replaced with Bayesian optimization)
        if len(self.trial_history) == 0:
            # Initial suggestion
            return {
                "rank": 8,
                "alpha": 16.0,
                "dropout": 0.1,
            }
        else:
            # Find best previous trial
            best_trial = max(
                self.trial_history, key=lambda t: t.get("score", 0.0)
            )
            # Perturb best parameters slightly
            import numpy as np

            params = {}
            for key, (low, high) in self.param_ranges.items():
                best_val = best_trial["params"].get(key, (low + high) / 2)
                # Add Gaussian noise
                noise = np.random.normal(0, (high - low) * 0.1)
                new_val = np.clip(best_val + noise, low, high)

                # Ensure rank is integer
                if key == "rank":
                    new_val = int(new_val)

                params[key] = new_val

            return params

    def record_trial(
        self, params: dict[str, Any], score: float, metrics: dict[str, float]
    ) -> None:
        """Record trial results.

        Args:
            params: Hyperparameters used
            score: Performance score (higher is better)
            metrics: Additional metrics
        """
        self.trial_history.append(
            {"params": params, "score": score, "metrics": metrics}
        )

    def get_best_params(self) -> dict[str, Any]:
        """Get best hyperparameters found so far.

        Returns:
            Best hyperparameters
        """
        if len(self.trial_history) == 0:
            return self.suggest_hyperparameters()

        best_trial = max(
            self.trial_history, key=lambda t: t.get("score", 0.0)
        )
        return best_trial["params"]


def auto_select_peft_method(
    task_features: mx.array,
    dataset_size: int,
    available_memory: float | None = None,
) -> str:
    """Automatically select best PEFT method for task.

    Args:
        task_features: Task embedding or features
        dataset_size: Number of training examples
        available_memory: Available GPU memory in GB (optional)

    Returns:
        Recommended PEFT method name
    """
    # Heuristic selection rules
    if dataset_size < 100:
        # Very few examples: use prompt tuning (fewest parameters)
        return "prompt_tuning"
    elif dataset_size < 1000:
        # Few examples: use LoRA (good balance)
        return "lora"
    elif available_memory and available_memory < 8.0:
        # Low memory: use AdaLoRA (adaptive rank)
        return "adalora"
    else:
        # Larger datasets: use standard LoRA
        return "lora"
