# Default configuration for CLIP Fine-tuning
# This configuration provides sensible defaults for domain-specific CLIP fine-tuning

# Model Configuration
model_name: "openai/clip-vit-base-patch32"  # HuggingFace model identifier
domain: "general"  # Domain for fine-tuning: general, medical, industrial, scientific

# Training Hyperparameters
learning_rate: 5.0e-5  # Learning rate for optimizer
batch_size: null  # Batch size (null = auto-determined based on available memory)
num_epochs: 10  # Number of training epochs
max_sequence_length: 77  # Maximum sequence length for text tokenization
image_resolution: 224  # Target resolution for input images

# Hardware Optimization
use_mps: true  # Use MPS backend for Apple Silicon GPU acceleration
mixed_precision: true  # Use mixed precision training (float16)
gradient_accumulation_steps: 1  # Number of gradient accumulation steps

# Output and Logging
output_dir: "outputs"  # Directory for saving checkpoints and logs

# Advanced Training Parameters
warmup_steps: 500  # Number of warmup steps for learning rate scheduler
weight_decay: 0.01  # Weight decay for optimizer
max_grad_norm: 1.0  # Maximum gradient norm for gradient clipping
save_steps: 1000  # Save checkpoint every N steps
eval_steps: 500  # Evaluate model every N steps
logging_steps: 100  # Log metrics every N steps
seed: 42  # Random seed for reproducibility

# Domain-Specific Configurations
# Uncomment and modify based on your domain

# Medical Domain
# domain: "medical"
# learning_rate: 3.0e-5
# num_epochs: 15

# Industrial Domain
# domain: "industrial"
# learning_rate: 5.0e-5
# num_epochs: 12

# Scientific Domain
# domain: "scientific"
# learning_rate: 4.0e-5
# num_epochs: 10
